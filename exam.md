# 1. Данные и метаданные; пример. 

Данные - таблица, значения, признаки

Метаданные – названия столбцов, способы измерения признаков, названия объектов и информация, о признаках

Пример: ирисы (150 * 4)

# 2. Признак как математическое понятие: два взгляда, два определения. 

Признак – отображение из множества объектов в множество значений

Признак – случайная величина

# 3. Понятие количественного признака в анализе данных. 

Признак, значение которого имеет признак суммировать / подсчитывать среднее значение

# 4. Функция плотности, примеры. 

Неотрицательная функция, $\int\limits_{-\infty}^{\infty} \rho(x) dx = 1$

Примеры:

# 5. Среднее значение и стандартное отклонение 

# 6. Степенная, Гауссова и равномерная функции плотности. 

# 7. Понятие номинального признака. 

Признак, (1) у которого малое число значений, (2) эти значения должны быть альтернативными (т.е. один объект может иметь только одно значение), (3) также он определен для всех объектов (т.е. неопределенное значение – тоже значение)

# 8. Бинарный признак и количественное представление номинального признака. 

Признак, который имеет всего два значения, обычно «да» / «нет». В рамках наших задач считаем количественным, потому что среднее бинарного признака – это частота ответа «да».

# 9. Понятие гистограммы. 

Представление признака в виде функции плотности. Для этого .. разбивается на n интервалов, обычно = 10, интервал называется бином. Подсчитываем количество наблюдений, попавших в интервал и рисуем столбик соответствующей высоты.

# 10. Корреляционное поле (scatterplot). 

Представление распределения двух признаков в виде точек на декартовой плоскости этих признаков.

# 11. Задача линейной регрессии и ее решение. 

Уравнение регрессии: $y = ax + b$

# 12. Коэффициент корреляции в задаче линейной регрессии, его свойства. 
![image](https://github.com/user-attachments/assets/42724774-5be6-4919-a811-2023f9d5c9a9)

# 13. Коэффициент детерминации, его смысл. 
Коэффициент детерминации – квадрат коэффициента корреляции. Показывает, какая доля дисперсии y учитывается уравнением регрессии
# 14. Привести пример взаимосвязанных признаков с нулевым коэффициентом корреляции. 
$y =  x^2 $
+ нарисовать график
# 15. Что можно сказать о коэффициенте корреляции между ростом и весом для группы мужчин из одной местности? 
Положительная: обычно, чем выше человек, тем его вес больше.
# 16. Корреляция между длиной и шириной чашелистика в данных Ирис отрицательна. Почему? 
Потому что ирисы состоят из трех таксонов. Внутри каждого таксона корреляция положительна, однако, из-за неоднородности общая корреляция отрицательна.
# 17. Ложная корреляция и парадокс Симпсона
Ложная корреляция возникает из-за неоднородности данных. Парадокс Симпсона – в каждом множестве корреляция одна, а в объединении множеств - обратная.
# 18. Можно ли значительно увеличить значение коэффициента корреляции в группе объектов, добавив один-два объекта? 
Да, если добавить объекты – выбросы.
# 19. Средняя относительная ошибка регрессионного прогноза в машинном обучении и в анализе данных. 
Средняя ошибка в ML:       (|e_i |)/((yc)_i )         где  yc – вычисленные значения y по формуле регрессии
Средняя ошибка в DA:       (|e_i |)/(y_i )          где  y – значения y из выборки
# 20. Коэффициент корреляции в вероятностной перспективе. 
$f(x,y) = const \exp (-\dfrac{1}{2} (x, y)$ ...
# 21. Искусственный нейрон; функция активации; сигмоида; гиперболический тангенс. 

# 22. Структура нейронной сети с внутренним слоем. 
# 23. Задача обучения нейронной сети.
# 24. Градиентный спуск. 
# 25. Метод обратного распространения ошибки. Что такое «эпоха»? 
# 26. Шаги метода линейной регрессии для многомерных наблюдений. 
# 27. Понятие ортогонального проектора. 

# 28. Что такое наивный Бейесовский классификатор? 
Априорное распределение - распределение до того, как мы учли данные таблицы
Апостериорное распределение - распределение после учета данных
Наивный Бейесовский классификатор - это правило для отнесения объектов к классам в предположении, что внутри каждого класса все признаки независимы

# 29. Оценка вероятностей ключевых слов в наивном Бейесовском классификаторе; модель мешка слов. 
У нас есть таблица со статьями (презентация про наивный Бейесовский классификатор), модель мешка слов - все вхождения слов суммируем и вероятность считаем как (кол-во вхождений конкретного слова) / (кол-во слов)

# 30. Зачем переходить к логарифмам вероятностей в наивном Бейесовском классификаторе? 
Потому что вероятности дают очень маленькие числа (близки к машинному нулю), операции над ними дадут неверные ответы

# 31. Основные понятия для оценки точности классификатора: прецизионность, полнота. 
Прецизионность (precision) = TP / (TP + TN)
Полнота (recall) = TP / (TP + FN)
Точность (accuracy) = (TP + TN) / (TP + TN + FP + FN)
F1 - гармоническое среднее recall и precision

# 32. Модель и метод главных компонент на основе сингулярного разложения прямоугольных матриц. 
Данные $Y = (y_{i,v})$, i - объект (1 ... N), v - признак (1 ... V)

Есть два фактора $f_1 = (f_{i,1}), f_2 = (f_{i,2})$ и два вектора нагрузок $c_1 = (c_{v,1}), c_2 = (c_{v,2})$

Модель главных компонент: 
Сделать систему
$y_{i,v} = \mu_1 f_{i,1} c_{v,1} + \mu_2 f_{i,2} c_{v,2} + e_{i,v}$
$||c_1|| = ||c_2|| = ||f_1|| = ||f_2|| = 1$
$\sum\limits_{i,v} e_{i,v}^2 \to \min$

Тогда:
$F = (f_1, f_2, \dots, f_r)$
$C = (c_1, c_2, \dots, c_r)$
$M = diag(\mu_1, \mu_2, \dots, \mu_r)$
r - ранг матрицы
$Y = FMC^{T}$
$\sum y_{i,v}^2 = \mu_1^2 + \mu_2^2 + \sum e_{i,v}^2 $

# 33. Сингулярные тройки и сингулярное разложение матрицы данных 
$(\mu \geq 0, z, c)$ называются сингулярной тройкой, если верна система $Yc = \mu z$ $Y^{T}z = \mu c$


# 34. Факторные баллы и нагрузки. 
Этого вопроса не будет :)

# 35. Вклад главной компоненты в разброс данных. 
$T = \sum y_{i,v}^2$

# 36. Визуализация данных на двумерной плоскости с помощью метода главных компонент. 
По методу главных компонент матрица оптимально представляется в двумерном пространстве, объекты представляются точками в пространстве с осями $\sqrt{\mu_1} f_1$ и $\sqrt{\mu_2} f_2$

# 37. Матрицы ковариации и корреляции. 
Рассмотрим матрицу Y. Сделаем ее центрированной (вычитаем из каждого столбца среднее значение по столбцу, после этого - среднее каждого столбца равен 0).

Теперь $B = (Y^{T} Y) / N$ - матрица ковариации.

$b_{k,l} = \dfrac{1}{N} \sum_i y_{i,k}y_{k,l}$

В случае, когда Y мы нормируем по z-scoring (вычитаем среднее, делим на стандартное отклонение), тогда B - матрица корреляции.

$b_{k,l} = \dfrac{\frac{1}{N} \sum_i y_{i,k}y_{k,l}}{\sigma_k, \sigma_l}$

$\sigma_k = \sqrt{\frac{1}{N} \sum_i y_i^2}$

# 38. Традиционный МГК (метод главных компонент на основе ковариационной матрицы). 
Описание метода из презентации
Этот метод строит такие компоненты z, которые учитывают максимально возможн ... данных

# 39. Сходство и различие модельного и традиционного подходов к МГК. 
Таблица сравнения из презентации

# 40. Зачем делают центрирование и нормализацию данных. 
Не будет этого вопроса

# 41. Связь сингулярных чисел матрицы данных с собственными числами ковариационной матрицы. 
Пусть $(\mu, z, c)$ - сингулярная тройка для матрицы Y, то есть $Yc = \mu z (1)$ & $Y^Tz = \mu c (2)$. Выразим z из (1), подставим в (2), домножим на $\mu$, разделим на N $\Rightarrow Bc = \dfrac{\mu ^ 2}{N} c$, т.е. $\lambda = \dfrac{\mu^2}{N} \Rightarrow \mu = \sqrt{\lambda N}$

# 42. Метод к-средних; входные и выходные данные. 
Входные данные - матрица данных, выходные данные - разбиение объектов на классы $S = \{1, \dots, K\}$ и вектор средних $c = \{c_1, \dots, c_K\}$

Метод: взять описание из презентации

# 43. Критерий метода к-средних. 
Сумма квадратов расстояния от объектов дл центров:
$D(S, c) = \sum\limits_{k=1}^K \sum\limits_{i \in S_k} d(i, c_k)$, где $d(i, c_k) = \sum_v (y_{i,v} - c_{v,k))^2$

# 44. Метод к-средних как чередующаяся минимизация. 
Нужно найти S, c, которые минимизируют $D(S,c)$.

# 45. Достоинства и недостатки метода к-средних. 
Вопроса не будет

# 46. Разложение Пифагора и вклад кластера в разброс данных. 
# 47. Дополнительный критерий для метода к-средних, его смысл. 
# 48. Метод аномального кластера. 
# 49. Интеллектуальная версия метода к-средних. 
# 50. Правила интерпретации кластера через относительную разность. 
# 51. Метод локтя (Elbow) для оценки числа кластеров. 
# 52. Смысл дополнительного критерия метода к-средних для случая номинальных данных. 
# 53. Методы МГК и к-средних как частные случаи матричной факторизации. 
# 54. Что такое таблица сопряженности. 
# 55. Условная вероятность и статистическая независимость. 
# 56. Коэффициент Кетле и его смысл; связь с правилами интерпретации кластеров. 
# 57. Средний коэффициент Кетле и его смысл. 
# 58. Коэффициент сопряженности Пирсона, его смысл. 
# 59. Связь между коэффициентом сопряженности Пирсона и средним коэффициентом Кетле. 
# 60. Использование метода бутстрэп для оценки доверительного интервала для среднего значения. 
# 61. Использование метода бутстрэп для сравнения средних. 
# 62. Чем отличаются методы бутстрэпа с опорой и без опоры. 
# 63. Что общего и что различного у методов дивизивного и агломеративного кластер-анализа? 
# 64. Определение расстояния Уарда между двумя кластерами через критерий метода к-средних и его переформулировка. 
# 65. Расстояние ближайшего соседа между двумя кластерами. 
# 66. Минимальное (максимальное) остовное дерево и алгоритм Прима для его построения. 
# 67. Дивизимный метод ближайшего соседа на основе минимального (максимального) остовного дерева. 
# 68. Агломеративный метод ближайшего соседа на основе минимального (максимального) остовного дерева. 
# 69. Критерий полусредней связи и его связь с методом к-средних. 
# 70. Критерий суммы связи в кластер-анализе; минимальный разрез и критерий модулярности. 
# 71. Агломеративный алгоритм и алгоритм Лувэн для кластер-анализа. 
# 72. Спектральные кластеры: нормализованный разрез, преобразование Лапласа и формирование спектальных кластеров.
