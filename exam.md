# 1. Данные и метаданные; пример. 

Данные - таблица, значения, признаки

Метаданные – названия столбцов, способы измерения признаков, названия объектов и информация, о признаках

Пример: ирисы (150 * 4)

# 2. Признак как математическое понятие: два взгляда, два определения. 

Признак – отображение из множества объектов в множество значений

Признак – случайная величина

# 3. Понятие количественного признака в анализе данных. 

Признак, значение которого имеет признак суммировать / подсчитывать среднее значение

# 4. Функция плотности, примеры. 

Неотрицательная функция, $\int\limits_{-\infty}^{\infty} \rho(x) dx = 1$

Примеры:

# 5. Среднее значение и стандартное отклонение 

# 6. Степенная, Гауссова и равномерная функции плотности. 

# 7. Понятие номинального признака. 

Признак, (1) у которого малое число значений, (2) эти значения должны быть альтернативными (т.е. один объект может иметь только одно значение), (3) также он определен для всех объектов (т.е. неопределенное значение – тоже значение)

# 8. Бинарный признак и количественное представление номинального признака. 

Признак, который имеет всего два значения, обычно «да» / «нет». В рамках наших задач считаем количественным, потому что среднее бинарного признака – это частота ответа «да».

# 9. Понятие гистограммы. 

Представление признака в виде функции плотности. Для этого .. разбивается на n интервалов, обычно = 10, интервал называется бином. Подсчитываем количество наблюдений, попавших в интервал и рисуем столбик соответствующей высоты.

# 10. Корреляционное поле (scatterplot). 

Представление распределения двух признаков в виде точек на декартовой плоскости этих признаков.

# 11. Задача линейной регрессии и ее решение. 

Уравнение регрессии: $y = ax + b$

# 12. Коэффициент корреляции в задаче линейной регрессии, его свойства. 
![image](https://github.com/user-attachments/assets/42724774-5be6-4919-a811-2023f9d5c9a9)

# 13. Коэффициент детерминации, его смысл. 
Коэффициент детерминации – квадрат коэффициента корреляции. Показывает, какая доля дисперсии y учитывается уравнением регрессии
# 14. Привести пример взаимосвязанных признаков с нулевым коэффициентом корреляции. 
$y =  x^2 $
+ нарисовать график
# 15. Что можно сказать о коэффициенте корреляции между ростом и весом для группы мужчин из одной местности? 
Положительная: обычно, чем выше человек, тем его вес больше.
# 16. Корреляция между длиной и шириной чашелистика в данных Ирис отрицательна. Почему? 
Потому что ирисы состоят из трех таксонов. Внутри каждого таксона корреляция положительна, однако, из-за неоднородности общая корреляция отрицательна.
# 17. Ложная корреляция и парадокс Симпсона
Ложная корреляция возникает из-за неоднородности данных. Парадокс Симпсона – в каждом множестве корреляция одна, а в объединении множеств - обратная.
# 18. Можно ли значительно увеличить значение коэффициента корреляции в группе объектов, добавив один-два объекта? 
Да, если добавить объекты – выбросы.
# 19. Средняя относительная ошибка регрессионного прогноза в машинном обучении и в анализе данных. 
Средняя ошибка в ML:       (|e_i |)/((yc)_i )         где  yc – вычисленные значения y по формуле регрессии
Средняя ошибка в DA:       (|e_i |)/(y_i )          где  y – значения y из выборки
# 20. Коэффициент корреляции в вероятностной перспективе. 
$f(x,y) = const \exp (-\dfrac{1}{2} (x, y)$ ...
# 21. Искусственный нейрон; функция активации; сигмоида; гиперболический тангенс. 

# 22. Структура нейронной сети с внутренним слоем. 
# 23. Задача обучения нейронной сети.
# 24. Градиентный спуск. 
# 25. Метод обратного распространения ошибки. Что такое «эпоха»? 
# 26. Шаги метода линейной регрессии для многомерных наблюдений. 
# 27. Понятие ортогонального проектора. 

# 28. Что такое наивный Бейесовский классификатор? 
Априорное распределение - распределение до того, как мы учли данные таблицы
Апостериорное распределение - распределение после учета данных
Наивный Бейесовский классификатор - это правило для отнесения объектов к классам в предположении, что внутри каждого класса все признаки независимы

# 29. Оценка вероятностей ключевых слов в наивном Бейесовском классификаторе; модель мешка слов. 
У нас есть таблица со статьями (презентация про наивный Бейесовский классификатор), модель мешка слов - все вхождения слов суммируем и вероятность считаем как (кол-во вхождений конкретного слова) / (кол-во слов)

# 30. Зачем переходить к логарифмам вероятностей в наивном Бейесовском классификаторе? 
Потому что вероятности дают очень маленькие числа (близки к машинному нулю), операции над ними дадут неверные ответы

# 31. Основные понятия для оценки точности классификатора: прецизионность, полнота. 
Прецизионность (precision) = TP / (TP + TN)
Полнота (recall) = TP / (TP + FN)
Точность (accuracy) = (TP + TN) / (TP + TN + FP + FN)
F1 - гармоническое среднее recall и precision

# 32. Модель и метод главных компонент на основе сингулярного разложения прямоугольных матриц. 
Данные $Y = (y_{i,v})$, i - объект (1 ... N), v - признак (1 ... V)

Есть два фактора $f_1 = (f_{i,1}), f_2 = (f_{i,2})$ и два вектора нагрузок $c_1 = (c_{v,1}), c_2 = (c_{v,2})$

Модель главных компонент: 
Сделать систему
$y_{i,v} = \mu_1 f_{i,1} c_{v,1} + \mu_2 f_{i,2} c_{v,2} + e_{i,v}$
$||c_1|| = ||c_2|| = ||f_1|| = ||f_2|| = 1$
$\sum\limits_{i,v} e_{i,v}^2 \to \min$

Тогда:
$F = (f_1, f_2, \dots, f_r)$
$C = (c_1, c_2, \dots, c_r)$
$M = diag(\mu_1, \mu_2, \dots, \mu_r)$
r - ранг матрицы
$Y = FMC^{T}$
$\sum y_{i,v}^2 = \mu_1^2 + \mu_2^2 + \sum e_{i,v}^2 $

# 33. Сингулярные тройки и сингулярное разложение матрицы данных 
$(\mu \geq 0, z, c)$ называются сингулярной тройкой, если верна система $Yc = \mu z$ $Y^{T}z = \mu c$


# 34. Факторные баллы и нагрузки. 
Этого вопроса не будет :)

# 35. Вклад главной компоненты в разброс данных. 
$T = \sum y_{i,v}^2$

# 36. Визуализация данных на двумерной плоскости с помощью метода главных компонент. 
По методу главных компонент матрица оптимально представляется в двумерном пространстве, объекты представляются точками в пространстве с осями $\sqrt{\mu_1} f_1$ и $\sqrt{\mu_2} f_2$

# 37. Матрицы ковариации и корреляции. 
Рассмотрим матрицу Y. Сделаем ее центрированной (вычитаем из каждого столбца среднее значение по столбцу, после этого - среднее каждого столбца равен 0).

Теперь $B = (Y^{T} Y) / N$ - матрица ковариации.

$b_{k,l} = \dfrac{1}{N} \sum_i y_{i,k}y_{k,l}$

В случае, когда Y мы нормируем по z-scoring (вычитаем среднее, делим на стандартное отклонение), тогда B - матрица корреляции.

$b_{k,l} = \dfrac{\frac{1}{N} \sum_i y_{i,k}y_{k,l}}{\sigma_k, \sigma_l}$

$\sigma_k = \sqrt{\frac{1}{N} \sum_i y_i^2}$

# 38. Традиционный МГК (метод главных компонент на основе ковариационной матрицы). 
Описание метода из презентации
Этот метод строит такие компоненты z, которые учитывают максимально возможн ... данных

# 39. Сходство и различие модельного и традиционного подходов к МГК. 
Таблица сравнения из презентации

# 40. Зачем делают центрирование и нормализацию данных. 
Не будет этого вопроса

# 41. Связь сингулярных чисел матрицы данных с собственными числами ковариационной матрицы. 
Пусть $(\mu, z, c)$ - сингулярная тройка для матрицы Y, то есть $Yc = \mu z (1)$ & $Y^Tz = \mu c (2)$. Выразим z из (1), подставим в (2), домножим на $\mu$, разделим на N $\Rightarrow Bc = \dfrac{\mu ^ 2}{N} c$, т.е. $\lambda = \dfrac{\mu^2}{N} \Rightarrow \mu = \sqrt{\lambda N}$

# 42. Метод к-средних; входные и выходные данные. 
Входные данные - матрица данных, выходные данные - разбиение объектов на классы $S = \{1, \dots, K\}$ и вектор средних $c = \{c_1, \dots, c_K\}$

Метод: взять описание из презентации

# 43. Критерий метода к-средних. 
Сумма квадратов расстояния от объектов дл центров:
$D(S, c) = \sum\limits_{k=1}^K \sum\limits_{i \in S_k} d(i, c_k)$, где $d(i, c_k) = \sum_v (y_{i,v} - c_{v,k})^2$

# 44. Метод к-средних как чередующаяся минимизация. 
Нужно найти S, c, которые минимизируют $D(S,c)$.

# 45. Достоинства и недостатки метода к-средних. 
Вопроса не будет

# 46. Разложение Пифагора и вклад кластера в разброс данных.
$D(S, c) = \sum\limits_{k=1}^K \sum\limits_{i \in S_k} d(i, c_k)$ - критерий метода к-средних

$F(S, c) = \sum\limits_{k} |S_k| <c_k, c_k>$ - дополнительный критерий метода к-средних

$T = F(S, c) + D(S, c)$ - разложение Пифагора

$|S_k| <c_k, c_k>$ - вклад кластера в разброс данных

# 47. Дополнительный критерий для метода к-средних, его смысл. 
$F(S, c) = \sum\limits_{k} |S_k| <c_k, c_k> \to \max$
Смысл: найти аномальные, большие кластеры

# 48. Метод аномального кластера. 
Цель: построить кластер с максимальным вкладом, т.е. $|S_k| <c_k, c_k> \to \max$
Метод:
1. Данные центрируются
2. Ищется объект, самый дальний от 0, этот объект назначается центром кластера = c
3. Формируется кластер S
4. Перерасчет центра = центр S
5. Проверка: (центр = с), если нет - повторяем с шага 3

# 49. Интеллектуальная версия метода к-средних. 
Ищется анамальный кластер, выкидывается, на остатке, не меняя 0, ищется следующий анамальный кластер, продолжаем, пока не разобьем все объекты. Выбираем к наибольших анамальных кластеров.

# 50. Правила интерпретации кластера через относительную разность. 
Расчитывается относительная разность: $c_k = (c_{1,k}, \dots, c_{v, k})$ - центр кластера, он сравнивается с $g = (g_1, \dots, g_v)$ - общим центром. $d_v = 100 * \dfrac{c_{v,k} - g_v}{g_v}$. Серьезным отклонением считаем отклонение >= 30%.

# 51. Метод локтя (Elbow) для оценки числа кластеров. 
Вставить из презентации

# 52. Смысл дополнительного критерия метода к-средних для случая номинальных данных. 
Не будет

# 53. Методы МГК и к-средних как частные случаи матричной факторизации. 
Не будет

# 54. Что такое таблица сопряженности. 
Таблица, строками которой являются категории одного признака, а столбцами - категории другого признака. На пересечении - количество объектов

Пример - Ирисы из презентации

# 55. Условная вероятность и статистическая независимость. 
Условная вероятность - это доля объектов одной категории внутри другой категории

Статистическая независимость - ситуация, при которой условная вероятность равна безусловной ($P(A | B) = P(A) \iff P(AB) = P(A)P(B) $)

В терминах таблицы сопряженности:  $p_{kl} = p_k * p_l$

# 56. Коэффициент Кетле и его смысл; связь с правилами интерпретации кластеров. 
Доля категории V в классе K, показывает насколько вероятность V учитывает, что мы находимся в кластере K.

$P(V|K) = \dfrac{P(VK)}{P(K)}$

$q(k|v) = q(v|k) = \dfrac{P(V|K)}{P(V)} - 1 = \dfrac{P(V|K) - P(V)}{P(V)}$

В интерпретации кластеров - это есть относительная разница между центрами кластеров и общим центром.

# 57. Средний коэффициент Кетле и его смысл. 
Насколько изменится шанс одной категории при изменении другой

$Q = \sum\limits_{v,k} p_{kv} q(v|k)$

# 58. Коэффициент сопряженности Пирсона, его смысл. 
$k+$ - сумма по строке k, $+v$ - сумма по столбцу v

$\varphi^2 = \sum_{k,v} \dfrac{(p_{kv} - p_{k+} * p_{+v})^2}{p_{k+}p_{+v}}$

$\chi ^2 = N \varphi^2$

Измеряет отклонение от независимости

# 59. Связь между коэффициентом сопряженности Пирсона и средним коэффициентом Кетле.

$Q = \chi^2$

# 60. Использование метода бутстрэп для оценки доверительного интервала для среднего значения.

Метод бутстрэп используется для формирования выборки любого заданного размера из имеющейся таблицы данных, для того, чтобы можно было применять статистические процедуры. Согласно этому методу, каждая новая выборка создается при помощи случайного выбора с возвращением. 

# 61. Использование метода бутстрэп для сравнения средних.
Не будет

# 62. Чем отличаются методы бутстрэпа с опорой и без опоры.

1) опора (pivotal) - есть нормальное распределение $N(m, \sigma) \Rightarrow 95%: [m - 1,96 \sigma, m + 1,96 \sigma]$ 
2) без опоры (non-pivotal) - "отрезаются" перцентили 2,5% и 97,5%, остается - 95%

# 63. Что общего и что различного у методов дивизивного и агломеративного кластер-анализа?
Не будет

# 64. Определение расстояния Уарда между двумя кластерами через критерий метода к-средних и его переформулировка.

$Wd = \dfrac{N_1N_2}{N_1 + N_2} d(c_1, c_2) = D (S_1 \cup S_2, S_3, \dots, S_K, c_{1 \cup 2}, c_3, \dots, c_K) - D (S_1, s_2, \dots, S_K, c_1, c_2, \dots, c_K)$

$D(S, c) = \sum_k \sum_{i \in S_k} d(i, c_k)$

# 65. Расстояние ближайшего соседа между двумя кластерами.

$NN(S_1, S_2) = \min_{i \in S_1, j \in S_2} d(i, j)$

# 66. Минимальное (максимальное) остовное дерево и алгоритм Прима для его построения.


# 67. Дивизимный метод ближайшего соседа на основе минимального (максимального) остовного дерева.


# 68. Агломеративный метод ближайшего соседа на основе минимального (максимального) остовного дерева.


# 69. Критерий полусредней связи и его связь с методом к-средних.


# 70. Критерий суммы связи в кластер-анализе; минимальный разрез и критерий модулярности.


# 71. Агломеративный алгоритм и алгоритм Лувэн для кластер-анализа. 


# 72. Спектральные кластеры: нормализованный разрез, преобразование Лапласа и формирование спектальных кластеров.
Не будет
