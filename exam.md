# 1. Данные и метаданные; пример. 

Данные - таблица, значения, признаки

Метаданные – названия столбцов, способы измерения признаков, названия объектов и информация, о признаках

Пример: ирисы (150 * 4)

# 2. Признак как математическое понятие: два взгляда, два определения. 

Признак – отображение из множества объектов в множество значений

Признак – случайная величина

# 3. Понятие количественного признака в анализе данных. 

Признак, значение которого имеет признак суммировать / подсчитывать среднее значение

# 4. Функция плотности, примеры. 

Неотрицательная функция, $\int\limits_{-\infty}^{\infty} \rho(x) dx = 1$

Примеры:

# 5. Среднее значение и стандартное отклонение 

# 6. Степенная, Гауссова и равномерная функции плотности. 

# 7. Понятие номинального признака. 

Признак, (1) у которого малое число значений, (2) эти значения должны быть альтернативными (т.е. один объект может иметь только одно значение), (3) также он определен для всех объектов (т.е. неопределенное значение – тоже значение)

# 8. Бинарный признак и количественное представление номинального признака. 

Признак, который имеет всего два значения, обычно «да» / «нет». В рамках наших задач считаем количественным, потому что среднее бинарного признака – это частота ответа «да».

# 9. Понятие гистограммы. 

Представление признака в виде функции плотности. Для этого .. разбивается на n интервалов, обычно = 10, интервал называется бином. Подсчитываем количество наблюдений, попавших в интервал и рисуем столбик соответствующей высоты.

# 10. Корреляционное поле (scatterplot). 

Представление распределения двух признаков в виде точек на декартовой плоскости этих признаков.

# 11. Задача линейной регрессии и ее решение. 

Уравнение регрессии: $y = ax + b$

# 12. Коэффициент корреляции в задаче линейной регрессии, его свойства. 
![image](https://github.com/user-attachments/assets/42724774-5be6-4919-a811-2023f9d5c9a9)

# 13. Коэффициент детерминации, его смысл. 
Коэффициент детерминации – квадрат коэффициента корреляции. Показывает, какая доля дисперсии y учитывается уравнением регрессии
# 14. Привести пример взаимосвязанных признаков с нулевым коэффициентом корреляции. 
$y =  x^2 $
+ нарисовать график
# 15. Что можно сказать о коэффициенте корреляции между ростом и весом для группы мужчин из одной местности? 
Положительная: обычно, чем выше человек, тем его вес больше.
# 16. Корреляция между длиной и шириной чашелистика в данных Ирис отрицательна. Почему? 
Потому что ирисы состоят из трех таксонов. Внутри каждого таксона корреляция положительна, однако, из-за неоднородности общая корреляция отрицательна.
# 17. Ложная корреляция и парадокс Симпсона
Ложная корреляция возникает из-за неоднородности данных. Парадокс Симпсона – в каждом множестве корреляция одна, а в объединении множеств - обратная.
# 18. Можно ли значительно увеличить значение коэффициента корреляции в группе объектов, добавив один-два объекта? 
Да, если добавить объекты – выбросы.
# 19. Средняя относительная ошибка регрессионного прогноза в машинном обучении и в анализе данных. 
Средняя ошибка в ML:       (|e_i |)/((yc)_i )         где  yc – вычисленные значения y по формуле регрессии
Средняя ошибка в DA:       (|e_i |)/(y_i )          где  y – значения y из выборки
# 20. Коэффициент корреляции в вероятностной перспективе. 
$f(x,y) = const \exp (-\dfrac{1}{2} (x, y)$ ...
# 21. Искусственный нейрон; функция активации; сигмоида; гиперболический тангенс. 

# 22. Структура нейронной сети с внутренним слоем. 
# 23. Задача обучения нейронной сети.
# 24. Градиентный спуск. 
# 25. Метод обратного распространения ошибки. Что такое «эпоха»? 
# 26. Шаги метода линейной регрессии для многомерных наблюдений. 
# 27. Понятие ортогонального проектора. 
# 28. Что такое наивный Бейесовский классификатор? 
# 29. Оценка вероятностей ключевых слов в наивном Бейесовском классификаторе; модель мешка слов. 
# 30. Зачем переходить к логарифмам вероятностей в наивном Бейесовском классификаторе? 
# 31. Основные понятия для оценки точности классификатора: прецизионность, полнота. 
# 32. Модель и метод главных компонент на основе сингулярного разложения прямоугольных матриц. 
# 33. Сингулярные тройки и сингулярное разложение матрицы данных 
# 34. Факторные баллы и нагрузки. 
# 35. Вклад главной компоненты в разброс данных. 
# 36. Визуализация данных на двумерной плоскости с помощью метода главных компонент. 
# 37. Матрицы ковариации и корреляции. 
# 38. Традиционный МГК (метод главных компонент на основе ковариационной матрицы). 
# 39. Сходство и различие модельного и традиционного подходов к МГК. 
# 40. Зачем делают центрирование и нормализацию данных. 
# 41. Связь сингулярных чисел матрицы данных с собственными числами ковариационной матрицы. 
# 42. Метод к-средних; входные и выходные данные. 
# 43. Критерий метода к-средних. 
# 44. Метод к-средних как чередующаяся минимизация. 
# 45. Достоинства и недостатки метода к-средних. 
# 46. Разложение Пифагора и вклад кластера в разброс данных. 
# 47. Дополнительный критерий для метода к-средних, его смысл. 
# 48. Метод аномального кластера. 
# 49. Интеллектуальная версия метода к-средних. 
# 50. Правила интерпретации кластера через относительную разность. 
# 51. Метод локтя (Elbow) для оценки числа кластеров. 
# 52. Смысл дополнительного критерия метода к-средних для случая номинальных данных. 
# 53. Методы МГК и к-средних как частные случаи матричной факторизации. 
# 54. Что такое таблица сопряженности. 
# 55. Условная вероятность и статистическая независимость. 
# 56. Коэффициент Кетле и его смысл; связь с правилами интерпретации кластеров. 
# 57. Средний коэффициент Кетле и его смысл. 
# 58. Коэффициент сопряженности Пирсона, его смысл. 
# 59. Связь между коэффициентом сопряженности Пирсона и средним коэффициентом Кетле. 
# 60. Использование метода бутстрэп для оценки доверительного интервала для среднего значения. 
# 61. Использование метода бутстрэп для сравнения средних. 
# 62. Чем отличаются методы бутстрэпа с опорой и без опоры. 
# 63. Что общего и что различного у методов дивизивного и агломеративного кластер-анализа? 
# 64. Определение расстояния Уарда между двумя кластерами через критерий метода к-средних и его переформулировка. 
# 65. Расстояние ближайшего соседа между двумя кластерами. 
# 66. Минимальное (максимальное) остовное дерево и алгоритм Прима для его построения. 
# 67. Дивизимный метод ближайшего соседа на основе минимального (максимального) остовного дерева. 
# 68. Агломеративный метод ближайшего соседа на основе минимального (максимального) остовного дерева. 
# 69. Критерий полусредней связи и его связь с методом к-средних. 
# 70. Критерий суммы связи в кластер-анализе; минимальный разрез и критерий модулярности. 
# 71. Агломеративный алгоритм и алгоритм Лувэн для кластер-анализа. 
# 72. Спектральные кластеры: нормализованный разрез, преобразование Лапласа и формирование спектальных кластеров.
